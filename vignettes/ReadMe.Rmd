---
title: "ReadMe: A Guid to sentiment Analysis with R step by step"
author: "Zahra Khoshmanesh"
date: "April 23, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Functions

### 1- Having Sence of Data 

In this function, user can have a quick view on data. The shiny version of this function fully implemented and can be use by user.

```{r}

#' Visualize dataset and get some insight of data.
#'
#' @param x dataset.
#' @return some diagram and insight of data
#' @author Zahra Khoshmanesh
#' @export
#' @import tidyverse
#' @examples
#' VisualizeData('./data/Restaurant_Reviews.tsv')


VisualizeData<-function(dataset){

  library(tidytext)
  library(dplyr)
  library(ggplot2)
  library(wordcloud)
  library(reshape2)

  source_datasets=read.delim(dataset,quote='',stringsAsFactors = FALSE)
  #source_datasets=read.delim('./inst/Restaurant_Reviews.tsv',quote='',stringsAsFactors = FALSE)


  text_df <- tibble(text = source_datasets[[1]])

  tidy_text <- text_df %>%
    unnest_tokens(word, text)

  data(stop_words)
  tidy_text <- tidy_text %>%
    anti_join(stop_words)




 wordfreqplot = tidy_text %>%
    count(word, sort = TRUE) %>%
    filter(n > 10) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()

 wordfreqplot



 wordcloadplot = tidy_text %>%
    anti_join(stop_words) %>%
    count(word) %>%
    with(wordcloud(word, n, max.words = 100))

 wordcloadplot



 reshapplot = tidy_text %>%
    inner_join(get_sentiments("bing")) %>%
    count(word, sentiment, sort = TRUE) %>%
    acast(word ~ sentiment, value.var = "n", fill = 0) %>%
    comparison.cloud(colors = c("gray20", "gray80"),
                     max.words = 100)

 reshapplot

}






```


### 2- Checking and fixing Unbalanced data

```{r}

#' Clean text and buil term matrix for bag of words model or TF DFI.
#'
#' @param x unbalanced dataset, a tsv format : tab delimeter, two column: first text and second binary class label.
#' @return balanced dataset, save as out.tsv name in inst folder
#' @author Zahra Khoshmanesh
#' @export
#' @import ROSE
#' @examples
#' BalanceData('./data/Imbalance_Restaurant_Reviews.tsv')


BalanceData<-function(dataset){

  library(ROSE)

  #read  datasets :put two dataset, 1 balance and 1 imbalanced for testing purpose
  source_datasets=read.delim(dataset,quote='',stringsAsFactors = FALSE)
  #source_datasets=read.delim('./inst/Imbalance_Restaurant_Reviews.tsv',quote='',stringsAsFactors = FALSE)

  #check the number of two class label
  check_class <-table(source_datasets[[-1]])

  res_name <- colnames(source_datasets)[ncol(source_datasets)]
  formula <- as.formula(paste(res_name, ' ~ .'))

    if (check_class[1]!=check_class[2]){

    print("dataset is imbalance, balancing it in few seconds")
    #data.rose <- ROSE(class_label ~ ., data = source_datasets, seed = 1)$data
    data.rose <- ROSE(formula, data = source_datasets, seed = 1)$data
    table(data.rose[[2]])

  }  else {
    print("dataset is balanced dataset and no need to balance it")
  }


  #save output in file
  write.table(data.rose, file='./inst/out.tsv', quote=FALSE, sep='\t', col.names = NA)

  if (file.access('./inst/out.tsv', mode = 0)==0){
    print("balancing dataset is done! and new balanced dataset saved in inst folder under name out.tsv ")

  }
}

#BalanceData('./data/Imbalance_Restaurant_Reviews.tsv')




```


### 3- Cleaning dataset and build term matrix for ML algorithms

```{r}

#' Clean text and build term matrix for bag of words,TF DFI and bi-gram.
#'
#' @param source_dataset A tsv file having two columns, review as text, label as binary.
#' @param dtm-method 1 for bag of word, 2 for TF DFI, 3 for bigram.
#' @param reductionrate how many percent of term matrix you want to keep,usually 0.999 and not less than 0.99.
#' @return dataframe "dataset" : The term matrix converted to dataframe plus target label.
#' @author Zahra Khoshmanesh
#' @export
#' @import tm
#' @import assertthat
#' @import testthat
#' @examples
#' CleanText('./data/Restaurant_Reviews.tsv',dtm_method=1,reductionrate=0.999)
#' CleanText('./data/Restaurant_Reviews.tsv',dtm_method=2,reductionrate=0.999)
#' CleanText('./data/Restaurant_Reviews.tsv',dtm_method=3,reductionrate=0.999)
CleanText <- function(source_dataset,dtm_method,reductionrate){

  #assertthat(not_empty(source_dataset), noNA(source_dataset),not_empty(dtm_method),not_empty(reductionrate))

  library(tm)
  library(rJava)
  library(RWeka)
  library(matlib)
  source_datasets=read.delim(source_dataset,quote='',stringsAsFactors = FALSE)
  corpus=VCorpus(VectorSource(source_datasets[[1]]))
  #convert all review to lower case
  corpus= tm_map(corpus,content_transformer(tolower))
  # remove numbers from reviews
  corpus=tm_map(corpus,removeNumbers)
  # remove punctuations from reviews
  corpus=tm_map(corpus,removePunctuation)
  # remove Stop words from reviews
  corpus=tm_map(corpus,removeWords,stopwords())
  # Stemming
  corpus=tm_map(corpus,stemDocument)
  # remove extra space that created in cleaning stage when for example number remove
  corpus=tm_map(corpus,stripWhitespace)

  #creating document term matrix of words in reviews

  # bigram
  BigramTokenizer <-  function(x)  unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
  dtm <-switch(dtm_method,
               '1' = DocumentTermMatrix(corpus),
               '2' = DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE))),
               '3' = t(TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer)))
                )

  # reduce dimention of sparse matrix
  dtm = removeSparseTerms(dtm,reductionrate)

  # convert matrix of independent variables to data frame
  dataset = as.data.frame(as.matrix(dtm))
  # encode the target feature as factor
  dataset$target = factor(source_datasets[[-1]],level=c(0,1))
  #assertthat(not_empty(dataset), noNA(dataset),is.data.frame(dataset))
  return(dataset)
}
#df<-CleanText('./data/Restaurant_Reviews.tsv',dtm_method=3,reductionrate=0.999)
#dim(df)



```


