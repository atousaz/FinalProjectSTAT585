---
title: "Workflow: SentiAnalyzer"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

### Sentiment analysis for consumer review

SentiAnalyzer is a "one-stop" solution for analysis of of consumer reviews which includes natural language processing (NLP) of consumer sentiments. The dataset that SentiAnalyzer can work with for now is short text and a binary quantification, e.g. whether consumer hits the like button. The NLP of the sentiment is analyzed through the options of algorithms to compile the words and the machinge learning training model of choice.  

## Installation
`install.packages("SentiAnalyzer")`

## Usage

Three steps of processing your review dataset: 

1. balancing the dataset
2. "streamlining the text to get a sense of the major keywords
3. choose the algorithm that is training the model 
4. Visualize the output of the confusion matrix, that is, the accuracy of the training model in predicting the sentiment of the consumer review


## tackling imbalanced data
The first step to process your data is to make sure that both of your variables have equal numbers of observations/unit. `BalanceData` function can be used to balance the called dataset. Using packages: `ROSE`, balance the columns of the data. For oversampled data, used `ovun.sample` to balance over or under sampled data. Then check the balance of the columns/variables.

`BalanceData` will automatically save a balanced dataset saved in `/data` folder under name `data.rose.Rda`

#### importing the imbalance dataset (OPTIONAL)
```{r include=FALSE}
imb <- system.file(package = "SentiAnalyzer", "extdata/Imbalance_Restaurant_Reviews.tsv")
imbalance_data <- read.delim(imb,quote='',stringsAsFactors = FALSE)
```

```{r message=FALSE, include=TRUE, warning=FALSE, results='hold'}
head(imbalance_data)
```

```{r message=FALSE, include=TRUE, warning=FALSE, results='hide', echo=TRUE}
SentiAnalyzer::BalanceData(imbalance_data)
```

check the balanced dataset, same number of rows and columns now

```{r include=FALSE}
direction <- system.file(package = "SentiAnalyzer", "extdata/Restaurant_Reviews.tsv")
balanced_data <- read.delim(direction,quote='',stringsAsFactors = FALSE)
```

```{r message=FALSE, include=TRUE, warning=FALSE, results='hold' }
dim(balanced_data)
str(balanced_data)
```


### Visualization
Before we start any further major processing of data, we can quickly get an insight of the data through a word cloud visualization. Termcount is used for filtering the highest terms repeated in reviews, usually > 10 count

**input**: a balanced dataset of text and binary sentiment review, low baseline for term count 

**output**: wordcloud plot and frequency of words bar plot
```{r message=FALSE, include=TRUE, warning=FALSE, results='hold', echo=TRUE, eval=FALSE}
VisualizeData(balanced_data,12)
```


## Cleaning the text
**input**: `CleanText()` calls for 3 arguments: 

a. the dataset 
b. document term matrix structure of choice (choose 1 from 3) 
c. reduction rate (range 0-1)

**output**: `CleanText()` returns a matrix `clean_dataset` saved as `data/clean_dataset.rda`

packages used: `tm` and `SnowballC`

##### a. text-mining the dataset
integrating built-in functions from `tm`, `CleanText()` will "clean up" the words from the text and mine for the words that conveys a range sorts of sentiment and convert some formatting; this remains what is called as token (single) or corpus (all the tokens):

*converts all text to lower case
*remove numbers from the text
*remove punctuations
*remove stop words, e.g. "the", "a", "for", "and", etc
*extract the stems of thegiven words using Porter's stemming algorithmn
*remove extra white spaces that was left off by the removed texts 

```{r message=FALSE, include=TRUE, warning=FALSE, results='hide', echo=TRUE}
library(tm)
```

```{r message=FALSE, include=TRUE, warning=FALSE, results='hold', echo=FALSE}
corpus=VCorpus(VectorSource(balanced_data$Review)) 
corpus=tm_map(corpus,content_transformer(tolower)) #convert all review to lower case
corpus=tm_map(corpus,removeNumbers) # remove numbers from reviews
corpus=tm_map(corpus,removePunctuation) # remove punctuations from reviews
corpus=tm_map(corpus,removeWords,stopwords()) # remove Stop words from reviews
corpus=tm_map(corpus,stemDocument) # Stemming
corpus=tm_map(corpus,stripWhitespace) # remove extra space that's created in cleaning stage above
```

```{r message=FALSE, include=TRUE, warning=FALSE, results='hold', echo=FALSE}
corpus$content[[1]][[1]]
corpus$content[[2]][[1]]
corpus$content[[3]][[1]]
corpus$content[[4]][[1]]
corpus$content[[5]][[1]]
corpus$content[[6]][[1]]
```

##### b. Creating the document-term matrix/bag of words model

Next, still within the scope of the same current function, `Cleantext()`, the corpus is formatted to a [document-term matrix](#https://en.wikipedia.org/wiki/Document-term_matrix) (DTM) and creating document term matrix of words in reviews. Essentially it creates a single column for every tokens in the corpus and counted for frequency of occurence on each tokens on the rows

user also have the choice to choose either (the argument call is also done on `CleanText`):

1. bag of words
2. tf-idf (term frequency-inverse document frequency)
3. Bi-gram 

##### c. choosing the reduction rate
the document-term matrix will quickly expand the dataset dimension, especially the sparse terms, significantly. Depending on the dimension of the dataset can be adjusted be adjusted within the range 0 to 1. Essentially it is calling `tm::removeSparseTerms`

```{r message=FALSE, include=TRUE, warning=FALSE, results='hide', echo=TRUE}
clean_dataset <- SentiAnalyzer::CleanText(balanced_data, dtm_method=1, reductionrate=0.99)
```
example:
```{r message=FALSE, include=TRUE, warning=FALSE, results='hold', echo=FALSE}
dim(clean_dataset)
names(clean_dataset)[1:50]
head(clean_dataset[1:10])
tail(clean_dataset[1:10])
```

## Building  classification

**input**: document-term matrix (result from `CleanText()` which is `clean_dataset`)

**output**: list of training model parameters from 4 machine learning, random forest algorithms

##### preparing the dataset and divide dataset to train and test
using package `caTools`

split the training and the test set to 0.8:0.2 split ratio, 0.8 of the dataset is used to train the model and 0.2 part is the predicted using the model

cross-validation????
```{r message=FALSE, include=TRUE, warning=FALSE, results='hide', echo=FALSE, eval=FALSE}
# convert matrix of independent variables to data frame
dataset = as.data.frame(as.matrix(dtm))

#dataset$targetclass = source_datasets[-1]
str(source_datasets[[-1]])

#dataset$Liked =  source_datasets$Liked

# encode the target feature as factor
dataset$target = factor(source_datasets[[-1]],level=c(0,1))
str(dataset$target)

# split dataset to training and test set
#install.packages('caTools')
library(caTools)
set.seed(123)
split=sample.split(dataset$target, SplitRatio = 0.8)
training_set=subset(dataset,split==TRUE)
test_set=subset(dataset,split==FALSE)
```

## Classifier : Random Forest
Training the model with various machine learning algorithms. classification that are popular for text mining, and are available in our package:

1. Decision Tree
2. Naive Bayes
3. k-Nearest Neighbors (kNN)
4. Gradient-Boosting Machines (GBM) 

The model training consist of `expand.grid()`, `train()` (formula, data, method, train control, tuneGrid, etc)

`randomForest` and `caret()` package

```{r message=FALSE, include=FALSE}
library(testthat)
library(assertthat)
library(caret)
```

Example shown is for Gradient Boosting Machine trained model 
```{r message=FALSE, include=TRUE, warning=FALSE, results='hold', echo=TRUE, eval=FALSE}
SentiAnalyzer::BuildTraining(clean_dataset)[[2]] 
```

```{r message=FALSE, include=FALSE, warning=FALSE, results='hide', echo=FALSE, eval=TRUE}
df_trained <- read.csv(system.file(package = "SentiAnalyzer", "extdata/testing.csv"))
```

### Build Prediction
predict the classes using trained algorithms and return list of confusion matrices

**input**: document-term matrix (result from `CleanText()` which is `clean_dataset`)

**output**: list of various parameter values of 4 machine learning algorithms

```{r message=FALSE, include=TRUE, warning=FALSE, results='hold', echo=TRUE, eval=FALSE}
df_predicted <- SentiAnalyzer::BuildPrediction(df_trained)
```


### Comparison of ML training models
among the 4 ML training model classifications 

**input**: document-term matrix (result from `CleanText()` which is `clean_dataset`)
  
**output**: list of confusion matrices of 4 machine learning algorithms
```{r message=FALSE, include=TRUE, warning=FALSE, results='hold', echo=TRUE, eval=TRUE}
Cmxx <- SentiAnalyzer::comparison(df_trained)
```

### Heatmap and table

**input**: confusion matrix from `Comparison`

**output**: interactive plotly heat map and table
```{r message=FALSE, warning=FALSE, echo=FALSE}
    Cmx_4param <- Cmxx[2:5]
    Cmx_4param <- as.data.frame(Cmx_4param) 
    colnames(Cmx_4param) <- c("accuracy","precision","recall","f1-score")
    rownames(Cmx_4param) <- c("Decision tree","Naive Bayes","k-Nearest Neighbors", "Gradient-Boosting")
    
    #heatmap encompassing 4 ML trained model output, 4 parameters of the confusion matrix
    plotly::plot_ly(z=data.matrix(Cmx_4param),
                    x=colnames(Cmx_4param),
                    y=rownames(Cmx_4param), 
                    type="heatmap",
                    textfont=list(size=20,color = "black"))
    
    #plotly table for 4 ML trained model output, 4 parameters of the confusion matrix
    plotly::plot_ly(
      type = 'table',
      header = list(
        values = c('<b>Confusion Matrix</b>', colnames(Cmx_4param)),
        line = list(color = '#506784'),
        fill = list(color = '#119DFF'),
        align = c('left','center'),
        font = list(color = 'white', size = 12)
      ),
      cells = list(
        values = rbind(rownames(Cmx_4param),
                       data.matrix(Cmx_4param)),
        line = list(color = '#506784'),
        fill = list(color = c('#25FEFD', 'white')),
        align = c('left', 'left'),
        font = list(color = c('#black'), size = 12)
        
        
      ))
```

