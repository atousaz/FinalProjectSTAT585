---
title: "Workflow"
author: "Joshua Budi"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r message=FALSE, include=FALSE, warning=FALSE, results='hold'}

```

```{r message=FALSE, include=FALSE, warning=FALSE, results='hide', echo=FALSE}
 
```



#### importing the imbalance dataset (OPTIONAL)
```{r message=FALSE, include=FALSE, warning=FALSE, results='hide', echo=FALSE}
imbalance_data<- read.delim(system.file(package = "SentiAnalyzer", "inst/extdata/Imbalance_Restaurant_Reviews.tsv"),quote='',stringsAsFactors = FALSE)
```

```{r message=FALSE, include=FALSE, warning=FALSE, results='hold'}
head(imbalance_data)
```

### tackling imbalanced data
The first step to process your data is to make sure that both of your variables have equal numbers of observations/unit. `BalanceData` function can be used to balance the called dataset. Using packages:`ROSE`, balance the columns of the data. For oversampled data, used `ovun.sample` to balance over or under sampled data. Then check the balance of the columns/variables.
`BalanceData` will automatically save a balanced dataset saved in `/data` folder under name `data.rose.Rda`

```{r message=FALSE, include=FALSE, warning=FALSE, results='hide', echo=TRUE}
SentiAnalyzer::BalanceData(imbalance_data)
```

check the balanced dataset, same number of rows and columns now
```{r message=FALSE, include=FALSE, warning=FALSE, results='hold' }
balanced_data<- read.delim(system.file(package = "SentiAnalyzer", "inst/extdata/out.tsv"),quote='',stringsAsFactors = FALSE)
str(balanced_data)
dim(balanced_data)

direction <- system.file(package = "SentiAnalyzer", "extdata/Restaurant_Reviews.tsv")
balanced_data <- read.delim(direction,quote='',stringsAsFactors = FALSE)
```

### Cleaning the texts
*input*:

`CleanText()` calls for 3 arguments: 
a. the dataset 
b. document term matrix structure of choice (choose 1 from 3) 
c. reduction rate (range 0-1)

*output*: `CleanText()` returns a matrix `clean_dataset` saved as `data/clean_dataset.rda`

packages used: `tm` and `SnowballC`

##### a. text-mining the dataset
integrating built-in functions from `tm`, `CleanText()` will "clean up" the words from the text and mine for the words that conveys a range sorts of sentiment and convert some formatting; this remains what is called as token (single) or corpus (all the tokens):
*converts all text to lower case
*remove numbers from the text
*remove punctuations
*remove stop words, e.g. "the", "a", "for", "and", etc
*extract the stems of thegiven words using Porter's stemming algorithmn
*remove extra white spaces that was left off by the removed texts 

```{r message=FALSE, include=FALSE, warning=FALSE, results='hold', echo=TRUE}
library(tm)
```

```{r message=FALSE, include=FALSE, warning=FALSE, results='hold', echo=FALSE}
corpus=VCorpus(VectorSource(balanced_data$Review)) 
corpus= tm_map(corpus,content_transformer(tolower)) #convert all review to lower case
corpus=tm_map(corpus,removeNumbers) # remove numbers from reviews
corpus=tm_map(corpus,removePunctuation) # remove punctuations from reviews
corpus=tm_map(corpus,removeWords,stopwords()) # remove Stop words from reviews
corpus=tm_map(corpus,stemDocument) # Stemming
corpus=tm_map(corpus,stripWhitespace) # remove extra space that's created in cleaning stage above
```

```{r message=FALSE, include=FALSE, warning=FALSE, results='hold', echo=FALSE}
corpus$content[[1]][[1]]
corpus$content[[2]][[1]]
corpus$content[[3]][[1]]
corpus$content[[4]][[1]]
corpus$content[[5]][[1]]
corpus$content[[6]][[1]]
```

##### b. Creating the document-term matrix/bag of words model
Next, still within the scope of the same current function, `Cleantext()`, the corpus is formatted to a [document-term matrix](#https://en.wikipedia.org/wiki/Document-term_matrix) (DTM) and creating document term matrix of words in reviews. Essentially it creates a single column for every tokens in the corpus and counted for frequency of occurence on each tokens on the rows

user also have the choice to choose either (the argument call is also done on CleanText) :
1. bag of words
2. tf-idf (term frequency-inverse document frequency)
3. Bi-gram 

##### c. choosing the reduction rate
the document-term matrix will quickly expand the dataset dimension, especially the sparse terms, significantly. Depending on the dimension of the dataset can be adjusted be adjusted within the range 0 to 1. Essentially it is calling `tm::removeSparseTerms`

```{r message=FALSE, include=FALSE, warning=FALSE, results='hide', echo=TRUE}
SentiAnalyzer::CleanText(balanced_data, dtm_method=1, reductionrate=0.99)
```
example:
```{r message=FALSE, include=FALSE, warning=FALSE, results='hold'}
dim(clean_dataset)
names(clean_dataset)[1:50]
clean_dataset[1:10]
```

### Building  classification
*input*: document-term matrix (result from `CleanText()` which is `clean_dataset`)
*output*: list of training model parameters from 4 machine learning, random forest algorithms

##### preparing the dataset and divide dataset to train and test
using package `caTools`
split the training and the test set to 0.8:0.2 split ratio, 0.8 of the dataset is used to train the model and 0.2 part is the predicted using the model

cross-validation????
```{r message=FALSE, include=FALSE, warning=FALSE, results='hide', echo=FALSE, eval=FALSE}
# convert matrix of independent variables to data frame
dataset = as.data.frame(as.matrix(dtm))

#dataset$targetclass = source_datasets[-1]
str(source_datasets[[-1]])

#dataset$Liked =  source_datasets$Liked

# encode the target feature as factor
dataset$target = factor(source_datasets[[-1]],level=c(0,1))
str(dataset$target)

# split dataset to training and test set
#install.packages('caTools')
library(caTools)
set.seed(123)
split=sample.split(dataset$target, SplitRatio = 0.8)
training_set=subset(dataset,split==TRUE)
test_set=subset(dataset,split==FALSE)
```

### classifier : Random Forest
Training the model with various machine learning algorithms. classification that are popular for text mining, and are available in our package:
1. Decision Tree
2. Naive Bayes
3. k-Nearest Neighbors (kNN)
4. Gradient-Boosting Machines (GBM) 

The model training consist of `expand.grid()`, `train()` (formula, data, method, train control, tuneGrid, etc)

`randomForest` and `caret()` package

```{r message=FALSE, include=FALSE, warning=FALSE, results='hide', echo=FALSE}
library(testthat)
library(assertthat)
library(caret)
```

Example shown is for Gradient Boosting Machine trained model 
```{r message=FALSE, include=FALSE, warning=FALSE, results='hold', echo=TRUE}
SentiAnalyzer::BuildTraining(clean_dataset)[[2]] 
```


### Build Prediction
predict the classes using trained algorithms and give confusion matrix  in return
*input*: document-term matrix (result from `CleanText()` which is `clean_dataset`)
*output*: list of various parameter values of 4 machine learning algorithms

```{r message=FALSE, include=FALSE, warning=FALSE, results='hide', echo=TRUE, eval=TRUE}
SentiAnalyzer::BuildPrediction(clean_dataset)[[2]]
```

### Comparison 
among the 4 ML training model classifications 
*input*: document-term matrix (result from `CleanText()` which is `clean_dataset`)
*output*: list of confusion matrices of 4 machine learning algorithms
```{r message=FALSE, include=FALSE, warning=FALSE, results='hide', echo=TRUE, eval=TRUE}
SentiAnalyzer::comparison(clean_dataset)
```





### Visualization

```{r message=FALSE, include=FALSE, warning=FALSE, results='hide', echo=FALSE, eval=TRUE}
VisualizeData<-function(dataset){

  library(tidyverse)
  library(tm)
  library(qdap)

  #source_datasets=read.delim(dataset,quote='',stringsAsFactors = FALSE)
  source_datasets=read.delim('./inst/Restaurant_Reviews.tsv',quote='',stringsAsFactors = FALSE)

  #check the number of two class label
  check_class <-table(source_datasets[[2]])
  class_label<-names(source_datasets)[2]
  class_text<-names(source_datasets)[1]
  if (check_class[1]!=check_class[2]){

    print("dataset is imbalance, balance it with calling BalanceData ")

  }  else {
    print("dataset is balanced dataset and no need to balance it")
  }

  corpus=VCorpus(VectorSource(source_datasets[[1]]))
  #convert all review to lower case
  corpus= tm_map(corpus,content_transformer(tolower))
  # remove numbers from reviews
  corpus=tm_map(corpus,removeNumbers)
  # remove punctuations from reviews
  corpus=tm_map(corpus,removePunctuation)
  # remove Stop words from reviews
  corpus=tm_map(corpus,removeWords,stopwords())
  # Stemming
  corpus=tm_map(corpus,stemDocument)
  # remove extra space that created in cleaning stage when for example number remove
  corpus=tm_map(corpus,stripWhitespace)
  myTDM <- TermDocumentMatrix(corpus)
  findFreqTerms(myTDM,lowfreq = 20, highfreq = Inf)



  a <- tolower(source_datasets[[1]])
  a <- removePunctuation(a)
  a <- removeNumbers(a)
  a <-rm_stopwords(a, tm::stopwords("english"))


  freq_term<-freq_terms(a)
  ## S3 method for class 'freq_terms'
  plot(freq_term, plot = TRUE)
  
  library(tidytext)
  library(dplyr)
  text_df <- tibble(text = source_datasets[[1]])
  
   tidy_text <- text_df %>%
    unnest_tokens(word, text)
  
  data(stop_words)
  tidy_text <- tidy_text %>%
    anti_join(stop_words)

  tidy_text  %>%
    count(word, sort = TRUE) 
  
  library(ggplot2)
  tidy_text %>%
    count(word, sort = TRUE) %>%
    filter(n > 10) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
}


```
